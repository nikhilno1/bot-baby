{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MDPXp5-X80r"
   },
   "source": [
    "# Download Twitter followers for a set of users\n",
    "\n",
    "### Notebook Author: Nikhil Utane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vp7x7kWeYABh",
    "outputId": "af1a20c2-2262-47f8-e27f-90076bd7860b",
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pip install GetOldTweets3 if you don't already have the package\n",
    "# !pip install GetOldTweets3\n",
    "\n",
    "# Imports\n",
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import time\n",
    "import tweepy\n",
    "import csv\n",
    "import sys\n",
    "from collections import Counter \n",
    "import requests\n",
    "import http.client, urllib\n",
    "import re\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the security tokens from a keys.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import keys #keep keys in separate file, keys.py\n",
    "\n",
    "consumer_key = keys['consumer_key']\n",
    "consumer_secret = keys['consumer_secret']\n",
    "access_token = keys['access_token']\n",
    "access_token_secret = keys['access_token_secret']\n",
    "pushover_token = keys['pushover_token']\n",
    "pushover_user = keys['pushover_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am using Pushover to notify me if any cell stops running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def pushoverNotify():\n",
    "    conn = http.client.HTTPSConnection(\"api.pushover.net:443\")\n",
    "    conn.request(\"POST\", \"/1/messages.json\",\n",
    "      urllib.parse.urlencode({\n",
    "        \"token\": pushover_token,\n",
    "        \"user\": pushover_user,\n",
    "        \"message\": \"Cell finished execution\",\n",
    "      }), { \"Content-type\": \"application/x-www-form-urlencoded\" })\n",
    "    r=conn.getresponse()\n",
    "    print(r.status, r.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Register the magic so that later any cell that we want to be notified on exception can be done\n",
    "@register_cell_magic('handle')\n",
    "def handle(line, cell):\n",
    "    try:\n",
    "        exec(cell)        \n",
    "    except Exception as e:\n",
    "        pushoverNotify()\n",
    "        raise # if you want the full trace-back in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get List of Followers. \n",
    "#### We are getting the IDs since the rate limit for that is quite high ~45000 per 15 mins vs ~3000 for usernames\n",
    "#### Then we'll convert ID to username and using GetOldTweets3 to download in bulk going as far back as 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the user configuration here\n",
    "side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_folder = \"../data/\" + side + \"/\"\n",
    "followers_folder = side_folder + \"followers/\"\n",
    "tweets_folder = side_folder + \"tweets/\"\n",
    "handles_file = side_folder + side + \"_handles.txt\"\n",
    "followers_id_file = side_folder + \"all_followers_id.txt\"\n",
    "followers_id_dedup_file = side_folder + \"all_followers_id_dedup.txt\"\n",
    "followers_username_file = side_folder + \"all_followers_username.txt\"\n",
    "fetched_username_files = tweets_folder + \"fetched_list.txt\"\n",
    "GetOldTweets3_bin = \"/home/nikhil/packages/GetOldTweets3/bin/GetOldTweets3\"\n",
    "processed_path = tweets_folder + \"processed/\"\n",
    "processed_tweets_file = processed_path + \"all_tweets.txt\"\n",
    "cleaned_tweets_file = processed_path + \"all_tweets_cleaned.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all follower IDs for a user using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Below source code credit: https://gist.github.com/PandaWhoCodes/46f58fdead71f4c71453d9ed1e21adf8\n",
    "# Credentials\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "def get_and_save_followers(user_name):\n",
    "    \"\"\"\n",
    "    get a list of all followers of a twitter account\n",
    "    :param user_name: twitter username without '@' symbol\n",
    "    :return: list of usernames without '@' symbol\n",
    "    \"\"\"\n",
    "    followers = []\n",
    "    with open(followers_folder + user_name + \"_followers_id.csv\", 'w',encoding=\"utf-8\") as output:\n",
    "        for page in tweepy.Cursor(api.followers_ids, screen_name=user_name, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True).pages():\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            try:\n",
    "                #followers.extend(page)\n",
    "                for user_id in page:\n",
    "                    output.write('%s\\n' % user_id)\n",
    "            except tweepy.TweepError as e:\n",
    "                print(\"Going to sleep:\", e)\n",
    "                # Sleeping to slow down. Else we hit rate limit often\n",
    "                time.sleep(60)\n",
    "    return followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the initial list of handles and get their follower IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%%handle\n",
    "\n",
    "with open(handles_file) as f:\n",
    "    handles = [line.rstrip() for line in f]\n",
    "    \n",
    "for handle in handles:\n",
    "    print(\"Getting followers for \" + handle)\n",
    "    followers = get_and_save_followers(handle)    \n",
    "    print(\"Done.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge, de-duplicate and sort the followers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/left/followers/\n",
      "../data/left/followers/all_followers_id.txt\n"
     ]
    }
   ],
   "source": [
    "print(followers_folder)\n",
    "print(followers_id_file)\n",
    "!echo $followers_folder/*.csv | xargs cat > $followers_id_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids before dedup: 15360681\n",
      "Number of ids after dedup: 8174646. Percent reduced: 53\n"
     ]
    }
   ],
   "source": [
    "# initializing list \n",
    "with open(followers_id_file) as f:\n",
    "    id_list = [line.rstrip() for line in f]\n",
    "\n",
    "# printing original list \n",
    "print(\"Number of ids before dedup: %d\" % len(id_list)) \n",
    "\n",
    "# using Counter.most_common() + list comprehension \n",
    "# sorting and removal of duplicates \n",
    "id_dedup = [key for key, value in Counter(id_list).most_common()] \n",
    "\n",
    "# print result \n",
    "print(\"Number of ids after dedup: {}. Percent reduced: {}\".format(len(id_dedup), int((len(id_dedup)*100)/len(id_list))) )\n",
    "\n",
    "with open(followers_id_dedup_file, \"w\") as output:\n",
    "    for user_id in id_dedup:\n",
    "        output.write('%s\\n' % user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert IDs to usernames for GetOldTweets3 to fetch in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%%handle\n",
    "# We are doing a GET on a twitter link and parsing our the username, fastest way with no rate limiting\n",
    "found = not_found = last_index = 0\n",
    "user_list = []\n",
    "\n",
    "# If you are resuming from somewhere in the middle, then uncomment below lines \n",
    "# and specify the last converted ID\n",
    "print(\"Reading ID file\");\n",
    "with open(followers_id_dedup_file) as f:\n",
    "    id_dedup = [line.rstrip() for line in f]\n",
    "\n",
    "last_index = id_dedup.index(\"1108670208607174656\")\n",
    "del id_dedup[0:last_index+1]\n",
    "\n",
    "with open(followers_username_file, \"w\") as output:\n",
    "    count = last_index + 1\n",
    "    for user_id in id_dedup:\n",
    "        print(\"[%d] Converting %s\" % (count, user_id) , end=' ');\n",
    "        r = requests.get('https://twitter.com/intent/user?user_id=' + user_id)\n",
    "        user_search=re.search('<title>.*\\(@(.*)\\).*</title>', r.content.decode('utf-8'), re.IGNORECASE)\n",
    "        if user_search:\n",
    "            username = user_search.group(1)            \n",
    "            user_list.append(username)\n",
    "            output.write('%s\\n' % username)\n",
    "            found += 1\n",
    "            print(\"=> %s\" % username);\n",
    "        else:\n",
    "            not_found += 1\n",
    "            print(\"ID %s not found\" % user_id);\n",
    "        count += 1\n",
    "        \n",
    "    print(\"%d usernames found. %d not found.\" % found, not_found)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GetOldTweets3 to download tweets upto Jan 2014 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Function the pulls tweets from a specific username and turns to csv file\n",
    "# Parameters: (list of twitter usernames), (max number of most recent tweets to pull from)\n",
    "def username_tweets_to_csv(username, count):\n",
    "    # Creation of query object\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                            .setSince(\"2014-01-01\")\\\n",
    "                                            .setMaxTweets(count)\\\n",
    "                                            .setEmoji(\"unicode\")\n",
    "    try:\n",
    "        # Creation of list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "        # Creating list of chosen tweet data\n",
    "        user_tweets = [[tweet.date, tweet.text] for tweet in tweets]\n",
    "\n",
    "        # Creation of dataframe from tweets list\n",
    "        tweets_df = pd.DataFrame(user_tweets, columns = ['Datetime', 'Text'])\n",
    "\n",
    "        # Converting dataframe to CSV\n",
    "        tweets_df.to_csv(tweets_folder + '/original/{}-{}k-tweets.csv'.format(username, int(count/1000)), sep=',')\n",
    "    except:\n",
    "        print(\"Caught Rate limit Exception. Sleeping...\")\n",
    "        time.sleep(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hide_output": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Reading followers\")\n",
    "with open(followers_username_file) as f:\n",
    "    user_list = [line.rstrip() for line in f] \n",
    "\n",
    "print(\"Reading already fetched usernames\")\n",
    "with open(fetched_username_files) as f:\n",
    "    fetched_list = [line.rstrip() for line in f]\n",
    "\n",
    "with open(fetched_username_files, \"a+\") as output:\n",
    "    count = 0\n",
    "    for username in user_list:    \n",
    "        if username not in fetched_list:\n",
    "            print(\"[%d] Fetching tweets for %s\" % (count, username))\n",
    "            username_tweets_to_csv(username, 0)    \n",
    "            output.write('%s\\n' % username)\n",
    "        else:\n",
    "            print(\"User %s already fetched. Skipping\" % username)\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture all tweets into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I downloaded tweets using GetOldTweets3 binary for original handles aka leaders and then using library for all followers. \n",
    "# The CSV format is different for both so make it same before we combine all tweets into a single file\n",
    "\n",
    "# Step 1) Set path below to appropriate folder\n",
    "# Step 2) Run the cell below to populate li\n",
    "# Step 3) Run the subsequent cell to generate either leaders_tweet_df or followers_tweet_df. \n",
    "# Step 4) Repeat above step if required\n",
    "# Step 5) Build the frames df that has all the tweets\n",
    "\n",
    "# Set the path one by one and run the below two cells appropriately to generate a dataframe of tweets\n",
    "#path = '/home/nikhil/packages/GetOldTweets3/bin/tweets/' + side \n",
    "path = tweets_folder + \"/original/followers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run one of the below - Temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaders_tweets_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_tweets_df = pd.concat(li, axis=0, ignore_index=True)\n",
    "# Tweets download using function call of GetOldTweets3 vs using the binary have different casing for column names\n",
    "# So convert below one to lowercase before merging\n",
    "followers_tweets_df.columns = followers_tweets_df.columns.str.lower()\n",
    "followers_tweets_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [leaders_tweets_df, followers_tweets_df]\n",
    "#frames = [leaders_tweets_df]\n",
    "all_tweets_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_df.text.to_csv(processed_tweets_file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Text Pre-processing - Cleanup tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: \n",
    "# 1) Remove URL\n",
    "# 2) Keep tweets greater than 20 characters\n",
    "\n",
    "MIN_CHARS = 20\n",
    "with open(processed_tweets_file) as f:\n",
    "    all_tweets = [line.rstrip() for line in f]    \n",
    "    \n",
    "    with open(cleaned_tweets_file, 'w',encoding=\"utf-8\") as output:\n",
    "        for tweet in all_tweets:            \n",
    "            tweet = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", tweet, flags=re.MULTILINE)\n",
    "            tweet = re.sub(\"([^\\x00-\\x7F])+\",\" \",tweet)\n",
    "            tweet = ' '.join(tweet.split()) \n",
    "            tweet = tweet.replace('&amp;', '&')            \n",
    "            if len(tweet) > MIN_CHARS:\n",
    "                output.write('%s\\n' % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle, De-duplicate and Split to train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable to get to work in Jupyter. Run from shell\n",
    "#!awk '!seen[$0]++' all_tweets_cleaned.txt > all_tweets_dedup.txt\n",
    "#!sed 's/\\\"//g' all_tweets_dedup.txt > all_tweets_dedup_2.txt # Remove all quotes\n",
    "#!sed 's/^ *//' all_tweets_dedup_2.txt > all_tweets_dedup_3.txt # Remove starting space\n",
    "\n",
    "# Not doing shuffle since I feel it is better to keep in order as tweets that are next to each other are more likely to be related.\n",
    "##!shuf all_tweets_dedup_3.txt > all_tweets_dedup_shuf.txt # <- Don't run\n",
    "\n",
    "#!rm all_tweets_cleaned.txt all_tweets_dedup.txt all_tweets_dedup_2.txt\n",
    "#!mv all_tweets_dedup_3.txt all_tweets_left.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download tweets using the GetOldTweets3 binary (appears to be doing it faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%handle\n",
    "\n",
    "# I used this to download tweets for all the 'leaders' (the original lef or right handles list)\n",
    "\n",
    "since_list=['2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01', '2020-01-01']\n",
    "until_list=['2014-12-31', '2015-12-31', '2016-12-31', '2017-12-31', '2018-12-31', '2019-12-31', '2020-12-31']\n",
    "\n",
    "# Set this for leaders or followers appropriately\n",
    "tweets_bin_folder = tweets_folder + \"original/leaders/\"\n",
    "#tweets_bin_folder = tweets_folder + \"original/followers/\"\n",
    "# Temp\n",
    "handles_file = side_folder + \"temp_handles.txt\"\n",
    "input_file = handles_file\n",
    "#input_file = followers_username_file\n",
    "    \n",
    "with open(input_file) as f:\n",
    "    handles = [line.rstrip() for line in f]\n",
    "    \n",
    "for handle in handles:\n",
    "    print(\"Getting tweets for \" + handle)\n",
    "    \n",
    "    for since, until in zip(since_list, until_list):\n",
    "        outfile = handle + \"_\" + since.split('-')[0] + \".csv\"\n",
    "        cmd = GetOldTweets3_bin + \" --username \" + handle + \\\n",
    "        \" --since \" + since + \" --until \" + until + \\\n",
    "        \" --maxtweets 0 --emoji unicode --output \" + tweets_bin_folder + outfile \n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "        print(\"%s Done.\" % outfile)\n",
    "        time.sleep(60)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some files are not downloaded properly. Re-download them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-download those files which have size 84 bytes\n",
    "tweets_bin_folder = tweets_folder + \"original/leaders/\"\n",
    "\n",
    "all_files = glob.glob(tweets_bin_folder + \"/*.csv\")\n",
    "\n",
    "for filename in all_files:\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size is 84:\n",
    "        outfile = os.path.basename(filename)\n",
    "        #print(\"%s is having size 84\" % filename)\n",
    "        handle=outfile.split('_20')[0]\n",
    "        year=outfile.split('.csv')[0][-4:]\n",
    "        #print(\"Re-fetching for %s & %s\" % (handle, year))\n",
    "        #outfile = handle + \"_\" + year + \".csv\"\n",
    "        cmd = GetOldTweets3_bin + \" --username \" + handle + \\\n",
    "        \" --since \" + year + \"-01-01\" + \" --until \" + year + \"-12-31\" + \\\n",
    "        \" --maxtweets 0 --emoji unicode --output \" + tweets_bin_folder + outfile\n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "        #cmd = \"mv \" + tweets_bin_folder + outfile + \" \" + tweets_bin_folder + \"../retried/\"\n",
    "        #os.system(cmd)\n",
    "        #print(cmd)\n",
    "        print(\"%s Done.\" % outfile)\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Unused Code/Scratchpad"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Not used for now\n",
    "def save_followers_to_csv(user_name, data):\n",
    "    \"\"\"\n",
    "    saves json data to csv\n",
    "    :param data: data recieved from twitter\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    HEADERS = [\"name\", \"screen_name\", \"description\", \"followers_count\", \"followers_count\",\n",
    "               'friends_count', \"listed_count\", \"favourites_count\", \"created_at\"]\n",
    "    with open(followers_folder + user_name + \"_followers.csv\", 'w',encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(HEADERS)\n",
    "        for profile_data in data:\n",
    "            profile = []\n",
    "            for header in HEADERS:\n",
    "                profile.append(profile_data._json[header])\n",
    "            csv_writer.writerow(profile)\n",
    "\n",
    "\n",
    "def save_followers_id_to_file(user_name, data):    \n",
    "    with open(followers_folder + user_name + \"_followers_id.csv\", 'w',encoding=\"utf-8\") as output:\n",
    "        for user_id in data:\n",
    "            output.write('%s\\n' % user_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "secrets = {}\n",
    "with open(\"/home/nikhil/my_projects/twitter_keys\") as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    for line in lines:\n",
    "        key, value = line.partition(\"=\")[::2]\n",
    "        secrets[key.strip()] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before dedup: 254462\n",
      "Number of words after dedup: 27099. Percent reduced: 10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('[^a-zA-Z\\']')\n",
    "#regex = re.compile('[,\\.!?]:-0-9') \n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "\n",
    "#Out: 'abdE'\n",
    "\n",
    "# initializing list \n",
    "words_file = \"1.txt\"\n",
    "with open(words_file) as f:\n",
    "    word_list = [line.rstrip() for line in f]\n",
    "\n",
    "# printing original list \n",
    "print(\"Number of words before dedup: %d\" % len(word_list)) \n",
    "\n",
    "# using Counter.most_common() + list comprehension \n",
    "# sorting and removal of duplicates \n",
    "word_dedup = [key for key, value in Counter(word_list).most_common()] \n",
    "\n",
    "# print result \n",
    "print(\"Number of words after dedup: {}. Percent reduced: {}\".format(len(word_dedup), int((len(word_dedup)*100)/len(word_list))) )\n",
    "\n",
    "with open(\"2.txt\", \"w\") as output:\n",
    "    for word in word_dedup:\n",
    "        new_word = regex.sub('', word)\n",
    "        if new_word:\n",
    "            output.write('%s\\n' % new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_file = \"../data/common/common.txt\"\n",
    "with open(common_file) as f:\n",
    "    common_list = [line.rstrip() for line in f]\n",
    "    for user in common_list:\n",
    "        cmd = \"mv ../data/right/tweets/original/followers/\" + user + \"* ../data/common/right/\" \n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GetOldTweets3 Twitter Scraper",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "twenv",
   "language": "python",
   "name": "twenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
